---
title: "Prediction Assignment Writeup"
author: "Nils o. Janus"
date: "December 9, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(h2o)
library(caret)
training <- read.csv("~/Desktop/tmp/pml-training.csv")
testing <- read.csv("~/Desktop/tmp/pml-testing.csv")
h2o.init()
```

# Data Preprocessing
Glancing at the raw data of the testing dataset, it readily becomes apparent that the testing data set contains a large number of columns with many NA values, which will not be used during the prediction.
Identifying and cleansing those columns in both the testing and traing data set is achieved as follows

```{r col_cleanse}
mostNA <- sapply(training, function(x) mean(is.na(x))) > 0.90
my_training <- training[, mostNA == FALSE]
my_testing <- testing[, mostNA == FALSE]
```

Since row 1 is just an ID, we can eliminate it as well
```{r final_cleanse}
my_training <- my_training[,-1]
my_testing <- my_testing[,-1]
```

# Model building
Since we are building a multiclass classifier, we build three different models, in order of increasing complexity:

1. Generalized Linear Model
2. Random Forest
3. Gradient Boosting Machines

for the model building part, we use the h2o library because of it's inbuilt support for multicore utilization.

K-fold cross validation (with K = 5) has been used for all models (this is the nfolds = 5 parameter in the h2o models)

```{r models, , results = FALSE}
h2o_train <- as.h2o(my_training)
h2o_test <- as.h2o(my_testing)
GLM <- h2o.glm(x = colnames(my_training)[1:57], y = "classe", training_frame = h2o_train, family = "multinomial", nfolds = 5)
DRF <- h2o.randomForest(x = colnames(my_training)[1:57], y = "classe", training_frame = h2o_train, ntrees = 100, nfolds = 5)
GBM <- h2o.gbm(x = colnames(my_training)[1:57], y = "classe", training_frame = h2o_train, nfolds = 5)
```

# Model performance and out of sample evaluation
Looking at the confusion matrices, we see that the performance of GLM is worst and that DRF and GBM score (almost) perfectly:
```{r confusionMatrices}
h2o.confusionMatrix(GLM)
h2o.confusionMatrix(DRF)
h2o.confusionMatrix(GBM)
```

Since K-fold cross validation is performed on the fly with h2o, we now split our training data into training and validation set, for estimating out of sample error:
```{r oos_setup}
inTrain <- createDataPartition(y=my_training$classe, p = 0.9, list = FALSE)
my_validattion <- my_training[-inTrain,]
h2o_valid <- as.h2o(my_validattion)
```

again, we see that estimated oos error is higest for GLM and lowest for GBM:
```{r oos}
h2o.performance(GLM, h2o_valid)
h2o.performance(DRF, h2o_valid)
h2o.performance(GBM, h2o_valid)
```

# Prediction
All models agree in their predictions for all of the twenty examples in the test set:
```{r prediction}
pred_glm <- h2o.predict(GLM, newdata = h2o_test)
pred_drf <- h2o.predict(DRF, newdata = h2o_test)
pred_gbm <- h2o.predict(GBM, newdata = h2o_test)

print(pred_glm[1],n=20)
print(pred_drf[1],n=20)
print(pred_gbm[1],n=20)
```

# Interpretation of results and explanation of choice
Seeing how all models agree on their final predictions on the testing set, either all of them hopelessly overfit or all of them are actually predicting the correct results with no or very little out of sample error. A short run with K-fold cross validation (K = 200) shows similar results, indicating that even a very small subset of data is sufficient to correctly predict results with a very low out of sample error.

The reason for choosing GLM, DRF and GBM is that they present increasingly more complexity (and thus lower interpretability) yet this added complexity allows for more accurate predictions. I consider it good style to start with computationally easy models first to obtain a baseline and gradually making them more expensive (= computationally hard). This approach has proven useful in this particular exercise as well.